{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KED_Bert_small.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1EhdcmP8bVhDtjtxuzUkXDayej_-9uv6K","authorship_tag":"ABX9TyPVlNV/F+CCEC3dvNP7kuHB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9630f796f9ce4e848e18ed5572b8065c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cabce3f821274b2d8ee6fb4d855376e1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a2fa9fae99324106a225ebaa961ed3f1","IPY_MODEL_d27bdf9c6d2f41b89baca6c4b66ada3d"]}},"cabce3f821274b2d8ee6fb4d855376e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a2fa9fae99324106a225ebaa961ed3f1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_847f9d7c177644ae9185840cf4740c0d","_dom_classes":[],"description":" 31%","_model_name":"FloatProgressModel","bar_style":"","max":17559,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5409,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0effc80e3a3c4df6bfad0a6c2be4bd59"}},"d27bdf9c6d2f41b89baca6c4b66ada3d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d862e0d1824a40ec8a1f1fc16ad1699a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5408/17559 [6:57:14&lt;16:13:19,  4.81s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ceebfffe4c7e4413abad5c1694156078"}},"847f9d7c177644ae9185840cf4740c0d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0effc80e3a3c4df6bfad0a6c2be4bd59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d862e0d1824a40ec8a1f1fc16ad1699a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ceebfffe4c7e4413abad5c1694156078":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"75826866339e4805b6f9ee1e94d44ffb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5eb3fa7ce71e40fda275297373094282","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_89e0b6e3a0d145c2a8d765e35fad7c6f","IPY_MODEL_ac214e9c827e4d9c9e5ecb8c1c55daa1"]}},"5eb3fa7ce71e40fda275297373094282":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"89e0b6e3a0d145c2a8d765e35fad7c6f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_48989cd372ca45adb7a0315669d75e2a","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":23412,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":23412,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9778bd2423d44e4dbb58ad0f6d1d811f"}},"ac214e9c827e4d9c9e5ecb8c1c55daa1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dbd79192f54349419cfd091b0725a7e4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 23412/23412 [2:07:50&lt;00:00,  3.05it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4313c08d99c54850aeb4ebc7d7ff533e"}},"48989cd372ca45adb7a0315669d75e2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9778bd2423d44e4dbb58ad0f6d1d811f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dbd79192f54349419cfd091b0725a7e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4313c08d99c54850aeb4ebc7d7ff533e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8af4ee90df134799a1801ca5f1247cb5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_24a639bd1ec7411882ebf40a8774fb8d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c48d6d05dea448f8b7b30a89fbc2fb21","IPY_MODEL_fa6fbfe66fa1469cb9c86ae8bea8234d"]}},"24a639bd1ec7411882ebf40a8774fb8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c48d6d05dea448f8b7b30a89fbc2fb21":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_767d8bf8147f48fe800f2e2d62b6353b","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":20030,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":20030,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cd8b91ba7d444f35a21c054ea0f45c58"}},"fa6fbfe66fa1469cb9c86ae8bea8234d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0661768276894520b776d39b3c2c367b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 20030/20030 [37:29&lt;00:00,  8.90it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_be5d889f6ad1474f9e7fed6672ad336b"}},"767d8bf8147f48fe800f2e2d62b6353b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cd8b91ba7d444f35a21c054ea0f45c58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0661768276894520b776d39b3c2c367b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"be5d889f6ad1474f9e7fed6672ad336b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aa804cedad1b4ba7bd9bde4694367b65":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a6cb94915b614a7db1b26aed2bcee28c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_18c9b58211ec4d49998b94a432836e1b","IPY_MODEL_2898ebb186f64bd7a36333d7ac5ba386"]}},"a6cb94915b614a7db1b26aed2bcee28c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"18c9b58211ec4d49998b94a432836e1b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8390d5a70fc54cad8721bc2c9ef74c19","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":23412,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":23412,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_63d562c2cb0e44439cc3fbddc3d284b1"}},"2898ebb186f64bd7a36333d7ac5ba386":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_173042df2e64425cb858702c4114d6a0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 23412/23412 [2:07:25&lt;00:00,  3.06it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0716f66564aa45fb9d0dc75ca511c59c"}},"8390d5a70fc54cad8721bc2c9ef74c19":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"63d562c2cb0e44439cc3fbddc3d284b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"173042df2e64425cb858702c4114d6a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0716f66564aa45fb9d0dc75ca511c59c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"87147c72e2db474f8600124efde6dae1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_52fc39666a374527810264bad0a20e2f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_978d94128ca5458b9f21d5284d88987a","IPY_MODEL_b007658d62154708827e407669390351"]}},"52fc39666a374527810264bad0a20e2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"978d94128ca5458b9f21d5284d88987a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2057b9111be74ca1a9647aff363978f9","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":20030,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":20030,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4bade703e5634d25a2758a04d0a70021"}},"b007658d62154708827e407669390351":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4f4e3391c982416cb596fb62cabbfba8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 20030/20030 [37:28&lt;00:00,  8.91it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f15ae028b9944375b69b88467b4717bd"}},"2057b9111be74ca1a9647aff363978f9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4bade703e5634d25a2758a04d0a70021":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f4e3391c982416cb596fb62cabbfba8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f15ae028b9944375b69b88467b4717bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aa837b8491f74722a804a7f593f1ccde":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6e558e01985746008c39fb6c1ec24515","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ecb992fa079f41f1b29445eb6d27a651","IPY_MODEL_23c9116f4cee462bbb283eb62811d5cc"]}},"6e558e01985746008c39fb6c1ec24515":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ecb992fa079f41f1b29445eb6d27a651":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4820e356a8d8420ea91693587fca03c3","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":599,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":599,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0f2490faf3db4c36b52b5e7f003e239b"}},"23c9116f4cee462bbb283eb62811d5cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ae0d299460d24e2885e7ab2218669e5d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 599/599 [01:26&lt;00:00,  6.93it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eb9b87b0209e43ecafbd274352d5a598"}},"4820e356a8d8420ea91693587fca03c3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0f2490faf3db4c36b52b5e7f003e239b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ae0d299460d24e2885e7ab2218669e5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eb9b87b0209e43ecafbd274352d5a598":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"31c519addcbf4094a1e3d6bbbe23b100":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b3fb9eaf49ba4f57a17a6db2bd7c5125","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9d51d38e34224d4fb2f4449f38fc66bd","IPY_MODEL_4377fbe447d44c3e9cc8106f5ed2a43c"]}},"b3fb9eaf49ba4f57a17a6db2bd7c5125":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9d51d38e34224d4fb2f4449f38fc66bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c7995b2b80f645feab9a5542a4ee04a9","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":15263,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":15263,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_98cd9383d93044a5a6aa1e81b1d52259"}},"4377fbe447d44c3e9cc8106f5ed2a43c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_59a2c70bde234fe6a8cfcaadc7cc9130","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 15263/15263 [37:22&lt;00:00,  6.81it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_361f4e2117284516afbd0f3ce1cbe5ad"}},"c7995b2b80f645feab9a5542a4ee04a9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"98cd9383d93044a5a6aa1e81b1d52259":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"59a2c70bde234fe6a8cfcaadc7cc9130":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"361f4e2117284516afbd0f3ce1cbe5ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RnDh2dZADgC5","executionInfo":{"status":"ok","timestamp":1621676452408,"user_tz":-540,"elapsed":16732,"user":{"displayName":"곽민지","photoUrl":"","userId":"00094324617217372282"}},"outputId":"87b843ce-72e0-4b43-fc53-24b466c6bdd7"},"source":["!pip install mxnet\n","!pip install gluonnlp\n","!pip install sentencepiece\n","!pip install transformers==3\n","!pip install torch\n","\n","!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: mxnet in /usr/local/lib/python3.7/dist-packages (1.8.0.post0)\n","Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.19.5)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet) (0.8.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n","Requirement already satisfied: gluonnlp in /usr/local/lib/python3.7/dist-packages (0.10.0)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.23)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (20.9)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.19.5)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (2.4.7)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n","Requirement already satisfied: transformers==3 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3) (3.0.12)\n","Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.8.0rc4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3) (20.9)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (4.41.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.1.95)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.0.45)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (8.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.0.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n","Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n","  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-dn1i6t02\n","  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-dn1i6t02\n","Requirement already satisfied (use --upgrade to upgrade): kobert==0.1.2 from git+https://****@github.com/SKTBrain/KoBERT.git@master in /usr/local/lib/python3.7/dist-packages\n","Building wheels for collected packages: kobert\n","  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert: filename=kobert-0.1.2-cp37-none-any.whl size=12708 sha256=2151646a8d423863f364b509f900541095ff7f471b07bd2c86ab36007a28000f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2hugnkqb/wheels/a2/b0/41/435ee4e918f91918be41529283c5ff86cd010f02e7525aecf3\n","Successfully built kobert\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":723,"referenced_widgets":["9630f796f9ce4e848e18ed5572b8065c","cabce3f821274b2d8ee6fb4d855376e1","a2fa9fae99324106a225ebaa961ed3f1","d27bdf9c6d2f41b89baca6c4b66ada3d","847f9d7c177644ae9185840cf4740c0d","0effc80e3a3c4df6bfad0a6c2be4bd59","d862e0d1824a40ec8a1f1fc16ad1699a","ceebfffe4c7e4413abad5c1694156078"]},"id":"v717qrPCyb3P","outputId":"4d60bb73-2c6a-42f5-9605-eddde919d256"},"source":["import pandas as pd\n","import os\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optims\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","import subprocess\n","\n","device = torch.device(\"cuda:0\")\n","\n","bertmodel, vocab = get_pytorch_kobert_model()\n","\n","\n","dataset_train = nlp.data.TSVDataset('/content/drive/MyDrive/애쓰는 감자/data/new_data/train_plus_small_2.txt', field_indices=[1,2], num_discard_samples=1)\n","# dataset_train = nlp.data.TSVDataset('/content/drive/MyDrive/애쓰는 감자/data/new_data/train_plus_small_2.txt', field_indices=[1,2], num_discard_samples=1)\n","# dataset_test = nlp.data.TSVDataset('/content/tst.txt', field_indices=[1,2], num_discard_samples=1)\n","\n","\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","\n","\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","\n","\n","max_len = 256 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n","batch_size = 32\n","warmup_ratio = 0.1\n","num_epochs = 2\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate = 5e-5\n","\n","\n","data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n","\n","\n","# pytorch용 DataLoader 사용\n","train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n","# test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)\n","\n","\n","\n","class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes = 231, # softmax 사용 <- binary일 경우는 2\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)\n","\n","\n","\n","# model = BERTClassifier(bertmodel, dr_rate=0.2).to(device)\n","model = torch.load('/content/drive/MyDrive/애쓰는 감자/data/minz_data/weight/best_kobert_model_small.pt')\n","\n","# if torch.cuda.device_count() > 1:\n","    # model = nn.DataParallel(model)\n","# model = nn.DataParallel(model, output_device=[0,1])\n","# model.to(device)\n","\n","\n","\n","# Prepare optimizer and schedule (linear warmup and decay)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","\n","\n","\n","\n","# 옵티마이저 선언\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss() # softmax용 Loss Function 정하기 <- binary classification도 해당 loss function 사용 가능\n","\n","t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)\n","\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n","\n","# lr_scheduler = optims.lr_scheduler.CosineAnnealingLR(optimizer,T_max=0.1,eta_min=0.0001)\n","\n","\n","# 학습 평가 지표인 accuracy 계산 -> 얼마나 타겟값을 많이 맞추었는가\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc\n","\n","\n","\n","# 모델 학습 시작\n","for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    best_acc = 0.0\n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # gradient clipping\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        # lr_scheduler.step()\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","        if train_acc > best_acc:\n","            best_acc = train_acc\n","        torch.save(model, '/content/drive/MyDrive/애쓰는 감자/data/minz_data/weight/best_kobert_small_2_0522.pt')\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","\n","\n","\n","torch.save(model, '/content/drive/MyDrive/애쓰는 감자/data/minz_data/weight/last_kobert_small_2_0522.pt')\n","\n","\n","# # model.eval() # 평가 모드로 변경\n","    \n","# # for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","# #     token_ids = token_ids.long().to(device)\n","# #     segment_ids = segment_ids.long().to(device)\n","# #     valid_length= valid_length\n","# #     label = label.long().to(device)\n","# #     out = model(token_ids, valid_length, segment_ids)\n","# #     test_acc += calc_accuracy(out, label)\n","# # print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["using cached model\n","using cached model\n","using cached model\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9630f796f9ce4e848e18ed5572b8065c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=17559.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch 1 batch id 1 loss 2.6538610458374023 train acc 0.5\n","epoch 1 batch id 201 loss 1.2830785512924194 train acc 0.6668221393034826\n","epoch 1 batch id 401 loss 0.8736941814422607 train acc 0.6671602244389028\n","epoch 1 batch id 601 loss 0.8923899531364441 train acc 0.6673772878535774\n","epoch 1 batch id 801 loss 1.5696357488632202 train acc 0.6629993757802747\n","epoch 1 batch id 1001 loss 1.1472407579421997 train acc 0.6624313186813187\n","epoch 1 batch id 1201 loss 1.0258311033248901 train acc 0.6626509159034139\n","epoch 1 batch id 1401 loss 1.970084309577942 train acc 0.6599527123483226\n","epoch 1 batch id 1601 loss 1.7736555337905884 train acc 0.659236414740787\n","epoch 1 batch id 1801 loss 1.0070611238479614 train acc 0.6586271515824542\n","epoch 1 batch id 2001 loss 2.0092527866363525 train acc 0.6582646176911544\n","epoch 1 batch id 2201 loss 1.2832001447677612 train acc 0.657045093139482\n","epoch 1 batch id 2401 loss 1.1184011697769165 train acc 0.6558595376926281\n","epoch 1 batch id 2601 loss 1.6162811517715454 train acc 0.6544478085351788\n","epoch 1 batch id 2801 loss 1.3747634887695312 train acc 0.6536058550517673\n","epoch 1 batch id 3001 loss 0.9626162052154541 train acc 0.6527615794735089\n","epoch 1 batch id 3201 loss 1.288009762763977 train acc 0.6522082942830365\n","epoch 1 batch id 3401 loss 1.682197093963623 train acc 0.6517935901205528\n","epoch 1 batch id 3601 loss 1.5844252109527588 train acc 0.6505658150513746\n","epoch 1 batch id 3801 loss 1.491039752960205 train acc 0.6501742962378322\n","epoch 1 batch id 4001 loss 0.9856109619140625 train acc 0.6492908022994252\n","epoch 1 batch id 4201 loss 1.555822730064392 train acc 0.6489079980956916\n","epoch 1 batch id 4401 loss 1.3033974170684814 train acc 0.6486309929561463\n","epoch 1 batch id 4601 loss 1.4862910509109497 train acc 0.648018093892632\n","epoch 1 batch id 4801 loss 1.127171516418457 train acc 0.6473911685065611\n","epoch 1 batch id 5001 loss 1.4715585708618164 train acc 0.647383023395321\n","epoch 1 batch id 5201 loss 1.168128490447998 train acc 0.6472373101326668\n","epoch 1 batch id 5401 loss 1.5145841836929321 train acc 0.6469403814108499\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0y_E78Vi1xd1"},"source":["train END"]},{"cell_type":"code","metadata":{"id":"Emq-odr1DoIp"},"source":["import pandas as pd\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tnetO85tDoKy","executionInfo":{"status":"ok","timestamp":1621658124286,"user_tz":-540,"elapsed":15304,"user":{"displayName":"곽민지","photoUrl":"","userId":"00094324617217372282"}},"outputId":"39f79f1e-05a2-46f6-829d-19e51ef22f91"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VShBaMquDoNS","executionInfo":{"status":"ok","timestamp":1621658366100,"user_tz":-540,"elapsed":187353,"user":{"displayName":"곽민지","photoUrl":"","userId":"00094324617217372282"}},"outputId":"000910d9-f2b6-48bf-f998-af6cbd42c34f"},"source":["device = torch.device(\"cuda:0\")\n","\n","bertmodel, vocab = get_pytorch_kobert_model()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[██████████████████████████████████████████████████]\n","[██████████████████████████████████████████████████]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7oCboTTgDoTG"},"source":["dataset_train = nlp.data.TSVDataset('/content/drive/MyDrive/애쓰는 감자/data/new_data/train_plus_small_1.txt', field_indices=[1,2], num_discard_samples=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FyxEdgTo6z4k"},"source":["# dataset_train = nlp.data.TSVDataset('/content/drive/MyDrive/애쓰는 감자/data/new_data/train_plus_small_2.txt', field_indices=[1,2], num_discard_samples=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wRiOrnNUFwtN"},"source":["dataset_val = nlp.data.TSVDataset('/content/drive/MyDrive/애쓰는 감자/data/new_data/val_small.txt', field_indices=[1,2], num_discard_samples=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BqxgsdE2DoVu","executionInfo":{"status":"ok","timestamp":1621658376458,"user_tz":-540,"elapsed":56,"user":{"displayName":"곽민지","photoUrl":"","userId":"00094324617217372282"}},"outputId":"0be659ac-6b9c-49c2-85fe-b369cbd88470"},"source":["tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["using cached model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YnxPSTQJDoYG"},"source":["class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R8nhHFEHD_u_"},"source":["max_len = 256 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n","batch_size = 50\n","warmup_ratio = 0.1\n","num_epochs = 2\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate = 5e-5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gfOb5QyqEBQ4"},"source":["data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n","data_val = BERTDataset(dataset_val, 0, 1, tok, max_len, True, False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FXIZpoMBEET5","executionInfo":{"status":"ok","timestamp":1621658683439,"user_tz":-540,"elapsed":39,"user":{"displayName":"곽민지","photoUrl":"","userId":"00094324617217372282"}},"outputId":"a00e9051-a392-4066-e92e-cf33089e8d34"},"source":["train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n","val_dataloader = torch.utils.data.DataLoader(data_val, batch_size=batch_size, num_workers=5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"FW6EfPEbEG_h"},"source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes = 231, # softmax 사용 <- binary일 경우는 2\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LIlBOHA9EI8R"},"source":["model = BERTClassifier(bertmodel, dr_rate=0.2).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XWRdgxIgEKhp"},"source":["# Prepare optimizer and schedule (linear warmup and decay)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oBbekeUEEMZa"},"source":["# 옵티마이저 선언\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss() # softmax용 Loss Function 정하기 <- binary classification도 해당 loss function 사용 가능\n","\n","t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)\n","\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C6TCWOHyEN0S"},"source":["# 학습 평가 지표인 accuracy 계산 -> 얼마나 타겟값을 많이 맞추었는가\n","\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OnXaDxBh-G8G"},"source":["# model = torch.load('/content/drive/MyDrive/애쓰는 감자/data/minz_data/best_kobert_model_small.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["75826866339e4805b6f9ee1e94d44ffb","5eb3fa7ce71e40fda275297373094282","89e0b6e3a0d145c2a8d765e35fad7c6f","ac214e9c827e4d9c9e5ecb8c1c55daa1","48989cd372ca45adb7a0315669d75e2a","9778bd2423d44e4dbb58ad0f6d1d811f","dbd79192f54349419cfd091b0725a7e4","4313c08d99c54850aeb4ebc7d7ff533e","8af4ee90df134799a1801ca5f1247cb5","24a639bd1ec7411882ebf40a8774fb8d","c48d6d05dea448f8b7b30a89fbc2fb21","fa6fbfe66fa1469cb9c86ae8bea8234d","767d8bf8147f48fe800f2e2d62b6353b","cd8b91ba7d444f35a21c054ea0f45c58","0661768276894520b776d39b3c2c367b","be5d889f6ad1474f9e7fed6672ad336b","aa804cedad1b4ba7bd9bde4694367b65","a6cb94915b614a7db1b26aed2bcee28c","18c9b58211ec4d49998b94a432836e1b","2898ebb186f64bd7a36333d7ac5ba386","8390d5a70fc54cad8721bc2c9ef74c19","63d562c2cb0e44439cc3fbddc3d284b1","173042df2e64425cb858702c4114d6a0","0716f66564aa45fb9d0dc75ca511c59c","87147c72e2db474f8600124efde6dae1","52fc39666a374527810264bad0a20e2f","978d94128ca5458b9f21d5284d88987a","b007658d62154708827e407669390351","2057b9111be74ca1a9647aff363978f9","4bade703e5634d25a2758a04d0a70021","4f4e3391c982416cb596fb62cabbfba8","f15ae028b9944375b69b88467b4717bd"]},"id":"HuBtp2vAEPIB","executionInfo":{"status":"ok","timestamp":1621403064119,"user_tz":-540,"elapsed":19816448,"user":{"displayName":"곽민지","photoUrl":"","userId":"00094324617217372282"}},"outputId":"6de5d06e-9a11-4c68-900d-23ccafb6a04d"},"source":["# 모델 학습 시작\n","for e in range(num_epochs):\n","    train_acc = 0.0\n","    val_acc = 0.0\n","    best_acc = 0.0\n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # gradient clipping\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","        if train_acc > best_acc:\n","           best_acc = train_acc\n","        torch.save(model, '/content/drive/MyDrive/애쓰는 감자/data/minz_data/weight/best_kobert_small_1_0522.pt')\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","\n","    model.eval()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        val_acc += calc_accuracy(out, label)\n","    print(\"epoch {} test acc {}\".format(e+1, val_acc / (batch_id+1)))\n","\n","    torch.save(model, '/content/drive/MyDrive/애쓰는 감자/data/minz_data/weight/last_kobert_small_1_0522.pt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  import sys\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"75826866339e4805b6f9ee1e94d44ffb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=23412.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["epoch 1 batch id 1 loss 2.5504603385925293 train acc 0.5416666666666666\n","epoch 1 batch id 201 loss 0.6707344651222229 train acc 0.6685323383084572\n","epoch 1 batch id 401 loss 1.4045881032943726 train acc 0.6626142975893597\n","epoch 1 batch id 601 loss 1.2170381546020508 train acc 0.664309484193012\n","epoch 1 batch id 801 loss 0.7562181353569031 train acc 0.6639617145235119\n","epoch 1 batch id 1001 loss 1.332964301109314 train acc 0.6617965367965366\n","epoch 1 batch id 1201 loss 1.1418415307998657 train acc 0.6627116291978901\n","epoch 1 batch id 1401 loss 1.2520266771316528 train acc 0.662324530097549\n","epoch 1 batch id 1601 loss 1.0380767583847046 train acc 0.6628149073495728\n","epoch 1 batch id 1801 loss 1.07827889919281 train acc 0.6619239311493613\n","epoch 1 batch id 2001 loss 1.4770020246505737 train acc 0.6616275195735465\n","epoch 1 batch id 2201 loss 0.9899908900260925 train acc 0.6612335302135391\n","epoch 1 batch id 2401 loss 1.138363242149353 train acc 0.6617381646536162\n","epoch 1 batch id 2601 loss 1.3249140977859497 train acc 0.661572472126105\n","epoch 1 batch id 2801 loss 0.8704274296760559 train acc 0.6617725812209927\n","epoch 1 batch id 3001 loss 0.559264063835144 train acc 0.6613351105187161\n","epoch 1 batch id 3201 loss 1.2633458375930786 train acc 0.6612516921795277\n","epoch 1 batch id 3401 loss 0.8387505412101746 train acc 0.660761540723318\n","epoch 1 batch id 3601 loss 1.3877569437026978 train acc 0.6603836897158206\n","epoch 1 batch id 3801 loss 1.272542953491211 train acc 0.6608239059896528\n","epoch 1 batch id 4001 loss 0.8443337082862854 train acc 0.6607618928601188\n","epoch 1 batch id 4201 loss 1.2533217668533325 train acc 0.6609041498056023\n","epoch 1 batch id 4401 loss 1.2736066579818726 train acc 0.6608819965159441\n","epoch 1 batch id 4601 loss 0.9061688780784607 train acc 0.6613145692965309\n","epoch 1 batch id 4801 loss 1.8292250633239746 train acc 0.6607651183781174\n","epoch 1 batch id 5001 loss 1.5573173761367798 train acc 0.660534559754717\n","epoch 1 batch id 5201 loss 0.828686535358429 train acc 0.6607783759533442\n","epoch 1 batch id 5401 loss 1.6289840936660767 train acc 0.6609038449669832\n","epoch 1 batch id 5601 loss 1.2808374166488647 train acc 0.6607674224840822\n","epoch 1 batch id 5801 loss 1.0008536577224731 train acc 0.6609348962822518\n","epoch 1 batch id 6001 loss 1.2533855438232422 train acc 0.6610842637338233\n","epoch 1 batch id 6201 loss 1.3246668577194214 train acc 0.660908186851584\n","epoch 1 batch id 6401 loss 0.7569754719734192 train acc 0.6606780190595223\n","epoch 1 batch id 6601 loss 1.0119346380233765 train acc 0.6609478361864368\n","epoch 1 batch id 6801 loss 1.3682808876037598 train acc 0.6609138362005598\n","epoch 1 batch id 7001 loss 1.3339849710464478 train acc 0.6612745798219305\n","epoch 1 batch id 7201 loss 1.3727432489395142 train acc 0.6610771652085354\n","epoch 1 batch id 7401 loss 1.1845413446426392 train acc 0.6610367968292565\n","epoch 1 batch id 7601 loss 1.2947393655776978 train acc 0.6609382537385429\n","epoch 1 batch id 7801 loss 1.4837301969528198 train acc 0.6609248814254575\n","epoch 1 batch id 8001 loss 0.9478927254676819 train acc 0.6609225930092064\n","epoch 1 batch id 8201 loss 1.5737324953079224 train acc 0.6610677559647187\n","epoch 1 batch id 8401 loss 0.963114321231842 train acc 0.6611861683132951\n","epoch 1 batch id 8601 loss 1.0548732280731201 train acc 0.6614104949036919\n","epoch 1 batch id 8801 loss 1.8169883489608765 train acc 0.6614683937431329\n","epoch 1 batch id 9001 loss 1.2601709365844727 train acc 0.6615144613561433\n","epoch 1 batch id 9201 loss 0.9079230427742004 train acc 0.661232474731005\n","epoch 1 batch id 9401 loss 1.1242111921310425 train acc 0.6613259227741708\n","epoch 1 batch id 9601 loss 1.2518733739852905 train acc 0.6610943304516875\n","epoch 1 batch id 9801 loss 1.3959627151489258 train acc 0.6611995374621623\n","epoch 1 batch id 10001 loss 1.0083681344985962 train acc 0.6613172016131708\n","epoch 1 batch id 10201 loss 1.759444236755371 train acc 0.6611933470574768\n","epoch 1 batch id 10401 loss 0.47941824793815613 train acc 0.6613025991090585\n","epoch 1 batch id 10601 loss 0.5933119058609009 train acc 0.6613330503411612\n","epoch 1 batch id 10801 loss 1.1501933336257935 train acc 0.6612466438292735\n","epoch 1 batch id 11001 loss 0.972223699092865 train acc 0.6611709541556811\n","epoch 1 batch id 11201 loss 1.4596885442733765 train acc 0.6610347290420477\n","epoch 1 batch id 11401 loss 1.089176058769226 train acc 0.6609727216910776\n","epoch 1 batch id 11601 loss 0.7631977200508118 train acc 0.6611463092262145\n","epoch 1 batch id 11801 loss 1.0663522481918335 train acc 0.6612539897748761\n","epoch 1 batch id 12001 loss 1.493135929107666 train acc 0.6612053162236462\n","epoch 1 batch id 12201 loss 0.6569992899894714 train acc 0.6611616534163846\n","epoch 1 batch id 12401 loss 1.1997498273849487 train acc 0.6611126790850176\n","epoch 1 batch id 12601 loss 1.8215622901916504 train acc 0.6613992275745278\n","epoch 1 batch id 12801 loss 1.5603727102279663 train acc 0.6614424654323847\n","epoch 1 batch id 13001 loss 1.4313092231750488 train acc 0.6614170704817544\n","epoch 1 batch id 13201 loss 1.485512375831604 train acc 0.6614618842006886\n","epoch 1 batch id 13401 loss 1.8180824518203735 train acc 0.6615551078277722\n","epoch 1 batch id 13601 loss 1.8543586730957031 train acc 0.6615628752787769\n","epoch 1 batch id 13801 loss 1.0139349699020386 train acc 0.6615492838683172\n","epoch 1 batch id 14001 loss 1.2943789958953857 train acc 0.661464657286383\n","epoch 1 batch id 14201 loss 1.093964695930481 train acc 0.6613296012017921\n","epoch 1 batch id 14401 loss 0.8359907269477844 train acc 0.6612908825775976\n","epoch 1 batch id 14601 loss 1.03426992893219 train acc 0.6612418099216939\n","epoch 1 batch id 14801 loss 0.9642334580421448 train acc 0.6612954079679294\n","epoch 1 batch id 15001 loss 1.7796880006790161 train acc 0.6613031353465316\n","epoch 1 batch id 15201 loss 1.4346404075622559 train acc 0.6612722847181096\n","epoch 1 batch id 15401 loss 1.4275215864181519 train acc 0.661309871653355\n","epoch 1 batch id 15601 loss 1.3062368631362915 train acc 0.661215627203384\n","epoch 1 batch id 15801 loss 1.532333493232727 train acc 0.6613320886863695\n","epoch 1 batch id 16001 loss 1.1671892404556274 train acc 0.6613284586380013\n","epoch 1 batch id 16201 loss 0.8550094962120056 train acc 0.6612914840647687\n","epoch 1 batch id 16401 loss 1.3664201498031616 train acc 0.661377355039326\n","epoch 1 batch id 16601 loss 0.9736236929893494 train acc 0.6613682910668024\n","epoch 1 batch id 16801 loss 0.5360373854637146 train acc 0.66133960280142\n","epoch 1 batch id 17001 loss 1.1062191724777222 train acc 0.6612993353332149\n","epoch 1 batch id 17201 loss 1.3291071653366089 train acc 0.6613593201945616\n","epoch 1 batch id 17401 loss 1.098257303237915 train acc 0.6613963756872197\n","epoch 1 batch id 17601 loss 1.1867777109146118 train acc 0.6614231198984888\n","epoch 1 batch id 17801 loss 1.4119375944137573 train acc 0.6613369099114261\n","epoch 1 batch id 18001 loss 0.9979650974273682 train acc 0.6614053848860231\n","epoch 1 batch id 18201 loss 1.3309828042984009 train acc 0.6613464461659603\n","epoch 1 batch id 18401 loss 1.4239377975463867 train acc 0.6612412368892983\n","epoch 1 batch id 18601 loss 1.2568745613098145 train acc 0.6613062917764266\n","epoch 1 batch id 18801 loss 2.398468255996704 train acc 0.6611549917557563\n","epoch 1 batch id 19001 loss 0.9150733947753906 train acc 0.6612020419977885\n","epoch 1 batch id 19201 loss 1.3409000635147095 train acc 0.6612611322326958\n","epoch 1 batch id 19401 loss 1.2776395082473755 train acc 0.6612545745064685\n","epoch 1 batch id 19601 loss 1.2372013330459595 train acc 0.6612864139584714\n","epoch 1 batch id 19801 loss 1.3795175552368164 train acc 0.6613639041799239\n","epoch 1 batch id 20001 loss 1.347738265991211 train acc 0.6613211006116366\n","epoch 1 batch id 20201 loss 1.284632921218872 train acc 0.6613265844925174\n","epoch 1 batch id 20401 loss 1.1068981885910034 train acc 0.6614075290426944\n","epoch 1 batch id 20601 loss 1.557275652885437 train acc 0.661474766597091\n","epoch 1 batch id 20801 loss 0.9918946623802185 train acc 0.6614485681778117\n","epoch 1 batch id 21001 loss 1.447136402130127 train acc 0.6614605653699028\n","epoch 1 batch id 21201 loss 1.7613496780395508 train acc 0.6615253997452949\n","epoch 1 batch id 21401 loss 0.6752505898475647 train acc 0.6614566297525032\n","epoch 1 batch id 21601 loss 1.5792707204818726 train acc 0.6614817215252373\n","epoch 1 batch id 21801 loss 0.8684672713279724 train acc 0.6615445774658644\n","epoch 1 batch id 22001 loss 0.6786060333251953 train acc 0.6616365922761079\n","epoch 1 batch id 22201 loss 1.3074687719345093 train acc 0.6614754590634047\n","epoch 1 batch id 22401 loss 1.276031255722046 train acc 0.6613748642173704\n","epoch 1 batch id 22601 loss 1.097072958946228 train acc 0.6613995693405885\n","epoch 1 batch id 22801 loss 1.091981291770935 train acc 0.6614402877066782\n","epoch 1 batch id 23001 loss 1.1633937358856201 train acc 0.6614495021955548\n","epoch 1 batch id 23201 loss 1.4519582986831665 train acc 0.6614334152263535\n","epoch 1 batch id 23401 loss 1.3120319843292236 train acc 0.6614532142501024\n","\n","epoch 1 train acc 0.6614503246198521\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8af4ee90df134799a1801ca5f1247cb5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=20030.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","epoch 1 test acc 0.6645824532432132\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa804cedad1b4ba7bd9bde4694367b65","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=23412.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch 2 batch id 1 loss 2.445823907852173 train acc 0.5833333333333334\n","epoch 2 batch id 201 loss 0.6696441769599915 train acc 0.6648009950248758\n","epoch 2 batch id 401 loss 1.3822304010391235 train acc 0.661783042394015\n","epoch 2 batch id 601 loss 1.2818067073822021 train acc 0.663893510815309\n","epoch 2 batch id 801 loss 0.7930278778076172 train acc 0.6643778610070753\n","epoch 2 batch id 1001 loss 1.3492579460144043 train acc 0.6617965367965376\n","epoch 2 batch id 1201 loss 1.1970771551132202 train acc 0.6630585623091874\n","epoch 2 batch id 1401 loss 1.3659133911132812 train acc 0.6629490839876286\n","epoch 2 batch id 1601 loss 0.9872567653656006 train acc 0.6628669581511561\n","epoch 2 batch id 1801 loss 1.0093976259231567 train acc 0.6617851193781247\n","epoch 2 batch id 2001 loss 1.4782794713974 train acc 0.6609820089955036\n","epoch 2 batch id 2201 loss 1.025614857673645 train acc 0.6602491291837054\n","epoch 2 batch id 2401 loss 1.1294909715652466 train acc 0.6609572400388739\n","epoch 2 batch id 2601 loss 1.3435357809066772 train acc 0.6610918877354874\n","epoch 2 batch id 2801 loss 0.8815988898277283 train acc 0.6613560633107233\n","epoch 2 batch id 3001 loss 0.6089338064193726 train acc 0.6610296567810738\n","epoch 2 batch id 3201 loss 1.2479321956634521 train acc 0.6610694574612113\n","epoch 2 batch id 3401 loss 0.9556431770324707 train acc 0.6607982946192307\n","epoch 2 batch id 3601 loss 1.4502090215682983 train acc 0.660175414236787\n","epoch 2 batch id 3801 loss 1.3136993646621704 train acc 0.6606923616592132\n","epoch 2 batch id 4001 loss 0.8385812640190125 train acc 0.6604286428392903\n","epoch 2 batch id 4201 loss 1.249459147453308 train acc 0.6603685630405465\n","epoch 2 batch id 4401 loss 1.218943476676941 train acc 0.660200333257594\n","epoch 2 batch id 4601 loss 0.8758449554443359 train acc 0.6605810331087452\n","epoch 2 batch id 4801 loss 1.8388968706130981 train acc 0.659827813649935\n","epoch 2 batch id 5001 loss 1.5026578903198242 train acc 0.6599013530627218\n","epoch 2 batch id 5201 loss 0.7781565189361572 train acc 0.6601134397231311\n","epoch 2 batch id 5401 loss 1.5244299173355103 train acc 0.660140097512807\n","epoch 2 batch id 5601 loss 1.2623142004013062 train acc 0.6601722906623823\n","epoch 2 batch id 5801 loss 1.001183271408081 train acc 0.6603746480491872\n","epoch 2 batch id 6001 loss 1.2245627641677856 train acc 0.6603899350108317\n","epoch 2 batch id 6201 loss 1.3360148668289185 train acc 0.6604378326076444\n","epoch 2 batch id 6401 loss 0.770697295665741 train acc 0.6603330208821543\n","epoch 2 batch id 6601 loss 0.9499210715293884 train acc 0.660783719638439\n","epoch 2 batch id 6801 loss 1.3951572179794312 train acc 0.6607239131500274\n","epoch 2 batch id 7001 loss 1.3886446952819824 train acc 0.6610722277769852\n","epoch 2 batch id 7201 loss 1.3127965927124023 train acc 0.6610655927417493\n","epoch 2 batch id 7401 loss 1.3176103830337524 train acc 0.6609410890420221\n","epoch 2 batch id 7601 loss 1.2657827138900757 train acc 0.6609601806779809\n","epoch 2 batch id 7801 loss 1.5263196229934692 train acc 0.6609035166431656\n","epoch 2 batch id 8001 loss 0.8863429427146912 train acc 0.6610007082448028\n","epoch 2 batch id 8201 loss 1.2722734212875366 train acc 0.6612404991261224\n","epoch 2 batch id 8401 loss 0.916175901889801 train acc 0.6614539935721935\n","epoch 2 batch id 8601 loss 1.1420310735702515 train acc 0.6618028911366893\n","epoch 2 batch id 8801 loss 1.9440408945083618 train acc 0.6618850130666966\n","epoch 2 batch id 9001 loss 1.2784854173660278 train acc 0.661829241195422\n","epoch 2 batch id 9201 loss 0.9201881885528564 train acc 0.6615856972068241\n","epoch 2 batch id 9401 loss 1.1807135343551636 train acc 0.6616627663723709\n","epoch 2 batch id 9601 loss 1.380690097808838 train acc 0.6613807589487205\n","epoch 2 batch id 9801 loss 1.4189486503601074 train acc 0.6615056286773462\n","epoch 2 batch id 10001 loss 1.0616987943649292 train acc 0.6616630003666304\n","epoch 2 batch id 10201 loss 1.811742901802063 train acc 0.6616467339803291\n","epoch 2 batch id 10401 loss 0.4314754903316498 train acc 0.6617432618658466\n","epoch 2 batch id 10601 loss 0.5446355938911438 train acc 0.6618125648523733\n","epoch 2 batch id 10801 loss 1.1890736818313599 train acc 0.6618021479492645\n","epoch 2 batch id 11001 loss 1.002421259880066 train acc 0.6616898463776031\n","epoch 2 batch id 11201 loss 1.4290375709533691 train acc 0.6615369163467559\n","epoch 2 batch id 11401 loss 1.11127769947052 train acc 0.661436862263545\n","epoch 2 batch id 11601 loss 0.7170445322990417 train acc 0.6616706893083937\n","epoch 2 batch id 11801 loss 1.1750690937042236 train acc 0.6617200519729971\n","epoch 2 batch id 12001 loss 1.4540005922317505 train acc 0.6617469377551878\n","epoch 2 batch id 12201 loss 0.5927413105964661 train acc 0.6616841515722761\n","epoch 2 batch id 12401 loss 1.1818159818649292 train acc 0.6615864312017854\n","epoch 2 batch id 12601 loss 1.7690926790237427 train acc 0.6618786868767037\n","epoch 2 batch id 12801 loss 1.5558557510375977 train acc 0.6619632580787966\n","epoch 2 batch id 13001 loss 1.4764386415481567 train acc 0.66196831012999\n","epoch 2 batch id 13201 loss 1.4506163597106934 train acc 0.6620173976718964\n","epoch 2 batch id 13401 loss 1.8716715574264526 train acc 0.6620650200233847\n","epoch 2 batch id 13601 loss 1.7810410261154175 train acc 0.6620836703183627\n","epoch 2 batch id 13801 loss 0.9627808928489685 train acc 0.66204441707123\n","epoch 2 batch id 14001 loss 1.3595656156539917 train acc 0.6619795014641849\n","epoch 2 batch id 14201 loss 1.1293175220489502 train acc 0.6617931835786243\n","epoch 2 batch id 14401 loss 0.9293932914733887 train acc 0.6617335601694357\n","epoch 2 batch id 14601 loss 0.9946839809417725 train acc 0.6616356185649411\n","epoch 2 batch id 14801 loss 0.9504995346069336 train acc 0.66175427335991\n","epoch 2 batch id 15001 loss 1.78018057346344 train acc 0.6618281003488673\n","epoch 2 batch id 15201 loss 1.3662382364273071 train acc 0.6617958248360863\n","epoch 2 batch id 15401 loss 1.5027570724487305 train acc 0.6618293184425263\n","epoch 2 batch id 15601 loss 1.3431367874145508 train acc 0.6617123902313966\n","epoch 2 batch id 15801 loss 1.639906883239746 train acc 0.6618172900449354\n","epoch 2 batch id 16001 loss 1.144219994544983 train acc 0.6617789513155442\n","epoch 2 batch id 16201 loss 0.9142351746559143 train acc 0.6617209843013822\n","epoch 2 batch id 16401 loss 1.4803513288497925 train acc 0.661776212832553\n","epoch 2 batch id 16601 loss 1.0193331241607666 train acc 0.661689556854008\n","epoch 2 batch id 16801 loss 0.5435646176338196 train acc 0.661639684145788\n","epoch 2 batch id 17001 loss 1.1133311986923218 train acc 0.661632649059862\n","epoch 2 batch id 17201 loss 1.3156895637512207 train acc 0.6616281999108612\n","epoch 2 batch id 17401 loss 1.0425482988357544 train acc 0.6617148439744871\n","epoch 2 batch id 17601 loss 1.152081847190857 train acc 0.6617379694335571\n","epoch 2 batch id 17801 loss 1.4244753122329712 train acc 0.6616411999325903\n","epoch 2 batch id 18001 loss 1.0703381299972534 train acc 0.6617386997018702\n","epoch 2 batch id 18201 loss 1.3376365900039673 train acc 0.6617310404190271\n","epoch 2 batch id 18401 loss 1.3634544610977173 train acc 0.6616510878032013\n","epoch 2 batch id 18601 loss 1.2918816804885864 train acc 0.6616467752629803\n","epoch 2 batch id 18801 loss 2.4348394870758057 train acc 0.6615250961828291\n","epoch 2 batch id 19001 loss 0.9385892748832703 train acc 0.661627458203956\n","epoch 2 batch id 19201 loss 1.344077467918396 train acc 0.6616365467076386\n","epoch 2 batch id 19401 loss 1.2145053148269653 train acc 0.6616647767297233\n","epoch 2 batch id 19601 loss 1.1810144186019897 train acc 0.6616286584017836\n","epoch 2 batch id 19801 loss 1.275404453277588 train acc 0.6617195259498697\n","epoch 2 batch id 20001 loss 1.3809171915054321 train acc 0.6617148309251225\n","epoch 2 batch id 20201 loss 1.3252462148666382 train acc 0.6617287923040133\n","epoch 2 batch id 20401 loss 1.1356942653656006 train acc 0.6618384719703294\n","epoch 2 batch id 20601 loss 1.6771248579025269 train acc 0.6618873679271241\n","epoch 2 batch id 20801 loss 1.0447221994400024 train acc 0.6618311619633677\n","epoch 2 batch id 21001 loss 1.4554901123046875 train acc 0.6618454676761429\n","epoch 2 batch id 21201 loss 1.8589286804199219 train acc 0.6618968444884679\n","epoch 2 batch id 21401 loss 0.6453054547309875 train acc 0.6618440727068845\n","epoch 2 batch id 21601 loss 1.6048911809921265 train acc 0.6617864913661416\n","epoch 2 batch id 21801 loss 0.9495487213134766 train acc 0.6618140605171028\n","epoch 2 batch id 22001 loss 0.664776086807251 train acc 0.6618998378861577\n","epoch 2 batch id 22201 loss 1.3937073945999146 train acc 0.6617907601759695\n","epoch 2 batch id 22401 loss 1.271103858947754 train acc 0.6617357112033705\n","epoch 2 batch id 22601 loss 1.088039755821228 train acc 0.6617719717416651\n","epoch 2 batch id 22801 loss 1.1275345087051392 train acc 0.6618240428051425\n","epoch 2 batch id 23001 loss 1.1763157844543457 train acc 0.6617900670985342\n","epoch 2 batch id 23201 loss 1.5301032066345215 train acc 0.6617369222591009\n","epoch 2 batch id 23401 loss 1.3327170610427856 train acc 0.6617576884178766\n","\n","epoch 2 train acc 0.6617493165897859\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"87147c72e2db474f8600124efde6dae1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=20030.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","epoch 2 test acc 0.6645824532432132\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SUUWFOeHjjYp"},"source":["<font color = pink> test 예측하기 </font>"]},{"cell_type":"code","metadata":{"id":"VeNPgKSkOSy0","colab":{"base_uri":"https://localhost:8080/","height":219,"referenced_widgets":["aa837b8491f74722a804a7f593f1ccde","6e558e01985746008c39fb6c1ec24515","ecb992fa079f41f1b29445eb6d27a651","23c9116f4cee462bbb283eb62811d5cc","4820e356a8d8420ea91693587fca03c3","0f2490faf3db4c36b52b5e7f003e239b","ae0d299460d24e2885e7ab2218669e5d","eb9b87b0209e43ecafbd274352d5a598"]},"executionInfo":{"status":"ok","timestamp":1621486499012,"user_tz":-540,"elapsed":111373,"user":{"displayName":"곽민지","photoUrl":"","userId":"00094324617217372282"}},"outputId":"7f80cf5d-ccc6-498b-fbac-95f546beb0af"},"source":["# test 예측하기\n","device = torch.device('cuda:0')\n","model = torch.load('/content/drive/MyDrive/애쓰는 감자/data/minz_data/best_kobert_model_small.pt')\n","model.to(device)\n","# model = nn.DataParallel(model, output_device=[0,1])\n","bertmodel, vocab = get_pytorch_kobert_model()\n","model.eval() # 평가 모드로 변경\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc\n","\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","max_len = 256 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n","batch_size = 32\n","warmup_ratio = 0.1\n","num_epochs = 2\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate = 5e-5\n","dataset_test = nlp.data.TSVDataset('/content/drive/MyDrive/애쓰는 감자/data/new_data/bert_test.txt', field_indices=[1,2], num_discard_samples=1)\n","data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n","test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)\n","test_acc = 0.0\n","df = pd.DataFrame(columns=['pred','label'])\n","pred = np.array([])\n","answer = np.array([])\n","for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","    token_ids = token_ids.long().to(device)\n","    segment_ids = segment_ids.long().to(device)\n","    valid_length= valid_length\n","    label = label.long().to(device)\n","    out = model(token_ids, valid_length, segment_ids)\n","    _,max_idx = torch.max(out,1)\n","    pred = np.append(pred,max_idx.cpu().detach().tolist())\n","    # answer = np.append(answer,label.cpu().detach().tolist())\n","    # test_acc += calc_accuracy(out, label)\n","    # print(len(pred))\n","df['pred'] = pred\n","# df['label'] = answer\n","\n","df.to_csv('/content/drive/MyDrive/애쓰는 감자/data/minz_data/test_small.csv',index=False)\n","# print(\"test acc {}\".format(test_acc / (batch_id+1)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["using cached model\n","using cached model\n","using cached model\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa837b8491f74722a804a7f593f1ccde","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=599.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rCE1gGbqj6pZ"},"source":["<font color = pink> val 예측하기 </font>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219,"referenced_widgets":["31c519addcbf4094a1e3d6bbbe23b100","b3fb9eaf49ba4f57a17a6db2bd7c5125","9d51d38e34224d4fb2f4449f38fc66bd","4377fbe447d44c3e9cc8106f5ed2a43c","c7995b2b80f645feab9a5542a4ee04a9","98cd9383d93044a5a6aa1e81b1d52259","59a2c70bde234fe6a8cfcaadc7cc9130","361f4e2117284516afbd0f3ce1cbe5ad"]},"id":"TUuOjtmxGd7s","executionInfo":{"status":"ok","timestamp":1621508947430,"user_tz":-540,"elapsed":2407431,"user":{"displayName":"곽민지","photoUrl":"","userId":"00094324617217372282"}},"outputId":"5f7190b5-07d0-4308-83d5-2944c261f04a"},"source":["# validation 예측하기\n","\n","device = torch.device('cuda:0')\n","model = torch.load('/content/drive/MyDrive/애쓰는 감자/data/minz_data/weight/best_kobert_model_small.pt')\n","model.to(device)\n","# model = nn.DataParallel(model, output_device=[0,1])\n","bertmodel, vocab = get_pytorch_kobert_model()\n","model.eval() # 평가 모드로 변경\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc\n","\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","max_len = 256 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n","batch_size = 32\n","warmup_ratio = 0.1\n","num_epochs = 2\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate = 5e-5\n","dataset_test = nlp.data.TSVDataset('/content/drive/MyDrive/애쓰는 감자/data/minz_data/val_small_pred.txt', field_indices=[1,2], num_discard_samples=1)\n","data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n","test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)\n","test_acc = 0.0\n","df = pd.DataFrame(columns=['pred','label'])\n","pred = np.array([])\n","answer = np.array([])\n","for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","    token_ids = token_ids.long().to(device)\n","    segment_ids = segment_ids.long().to(device)\n","    valid_length= valid_length\n","    label = label.long().to(device)\n","    out = model(token_ids, valid_length, segment_ids)\n","    _,max_idx = torch.max(out,1)\n","    pred = np.append(pred,max_idx.cpu().detach().tolist())\n","    # answer = np.append(answer,label.cpu().detach().tolist())\n","    # test_acc += calc_accuracy(out, label)\n","    # print(len(pred))\n","df['pred'] = pred\n","# df['label'] = answer\n","\n","df.to_csv('/content/drive/MyDrive/애쓰는 감자/data/minz_data/val_small_pred.csv',index=False)\n","# print(\"test acc {}\".format(test_acc / (batch_id+1)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["using cached model\n","using cached model\n","using cached model\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31c519addcbf4094a1e3d6bbbe23b100","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=15263.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IeOIgPwzUkFv"},"source":[""],"execution_count":null,"outputs":[]}]}