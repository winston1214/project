{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KED_Bert_mid.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1l1Wv9FawA5dIasEqSza_S-bKGgfLwSoI","authorship_tag":"ABX9TyM1AmudVGKbcBDWq6lf5e+u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7a53027283234d7a8641fecbcdeb7a79":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5a6d2bbfe6cf4020aaf15cab80605504","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5c79847a595a4d07a79c7515bab018c6","IPY_MODEL_1584c71aced14e208d251dfce3b1bc53"]}},"5a6d2bbfe6cf4020aaf15cab80605504":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5c79847a595a4d07a79c7515bab018c6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_429eef2bccd949d18c496768248d52d0","_dom_classes":[],"description":" 43%","_model_name":"FloatProgressModel","bar_style":"","max":17837,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":7610,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_39ff263fe7604ab1adeaecccf5b79c0e"}},"1584c71aced14e208d251dfce3b1bc53":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a152f69fd91348d1b568ba3c7cd7a45d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 7610/17837 [9:46:44&lt;12:52:49,  4.53s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9089e76a3e8d42ae98b44a42dd1ef3ed"}},"429eef2bccd949d18c496768248d52d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"39ff263fe7604ab1adeaecccf5b79c0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a152f69fd91348d1b568ba3c7cd7a45d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9089e76a3e8d42ae98b44a42dd1ef3ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aa837b8491f74722a804a7f593f1ccde":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6e558e01985746008c39fb6c1ec24515","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ecb992fa079f41f1b29445eb6d27a651","IPY_MODEL_23c9116f4cee462bbb283eb62811d5cc"]}},"6e558e01985746008c39fb6c1ec24515":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ecb992fa079f41f1b29445eb6d27a651":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4820e356a8d8420ea91693587fca03c3","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":599,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":599,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0f2490faf3db4c36b52b5e7f003e239b"}},"23c9116f4cee462bbb283eb62811d5cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ae0d299460d24e2885e7ab2218669e5d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 599/599 [01:26&lt;00:00,  6.93it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eb9b87b0209e43ecafbd274352d5a598"}},"4820e356a8d8420ea91693587fca03c3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0f2490faf3db4c36b52b5e7f003e239b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ae0d299460d24e2885e7ab2218669e5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eb9b87b0209e43ecafbd274352d5a598":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"31c519addcbf4094a1e3d6bbbe23b100":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b3fb9eaf49ba4f57a17a6db2bd7c5125","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9d51d38e34224d4fb2f4449f38fc66bd","IPY_MODEL_4377fbe447d44c3e9cc8106f5ed2a43c"]}},"b3fb9eaf49ba4f57a17a6db2bd7c5125":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9d51d38e34224d4fb2f4449f38fc66bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c7995b2b80f645feab9a5542a4ee04a9","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":15263,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":15263,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_98cd9383d93044a5a6aa1e81b1d52259"}},"4377fbe447d44c3e9cc8106f5ed2a43c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_59a2c70bde234fe6a8cfcaadc7cc9130","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 15263/15263 [37:22&lt;00:00,  6.81it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_361f4e2117284516afbd0f3ce1cbe5ad"}},"c7995b2b80f645feab9a5542a4ee04a9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"98cd9383d93044a5a6aa1e81b1d52259":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"59a2c70bde234fe6a8cfcaadc7cc9130":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"361f4e2117284516afbd0f3ce1cbe5ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RnDh2dZADgC5","executionInfo":{"status":"ok","timestamp":1621663939693,"user_tz":-540,"elapsed":31425,"user":{"displayName":"곽민지","photoUrl":"","userId":"00094324617217372282"}},"outputId":"9a1f3e41-f281-4b8f-8079-0f1d9e74cb86"},"source":["!pip install mxnet\n","!pip install gluonnlp\n","!pip install sentencepiece\n","!pip install transformers==3\n","!pip install torch\n","\n","!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting mxnet\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/07/66174e78c12a3048db9039aaa09553e35035ef3a008ba3e0ed8d2aa3c47b/mxnet-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (46.9MB)\n","\u001b[K     |████████████████████████████████| 46.9MB 109kB/s \n","\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n","Collecting graphviz<0.9.0,>=0.8.1\n","  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n","Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.19.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n","Installing collected packages: graphviz, mxnet\n","  Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.8.4 mxnet-1.8.0.post0\n","Collecting gluonnlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n","\u001b[K     |████████████████████████████████| 348kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.19.5)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.23)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (20.9)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (2.4.7)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595676 sha256=890ceb97dc8fa73b28fe9947e655830090cb5d4d89db86606b2af2f2c5319f93\n","  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.10.0\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 2.8MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n","Collecting transformers==3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n","\u001b[K     |████████████████████████████████| 757kB 2.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3) (20.9)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.1.95)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 17.0MB/s \n","\u001b[?25hCollecting tokenizers==0.8.0-rc4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/82/0e82a95bd9db2b32569500cc1bb47aa7c4e0f57aa5e35cceba414096917b/tokenizers-0.8.0rc4-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 18.8MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (1.24.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (8.0.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.0.1)\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.8.0rc4 transformers-3.0.0\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n","  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-36t_52n4\n","  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-36t_52n4\n","Building wheels for collected packages: kobert\n","  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert: filename=kobert-0.1.2-cp37-none-any.whl size=12708 sha256=c80d6d4159420152a1d904453bdd058a5db37441cacca2520c00e5e828a675dc\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ij81l982/wheels/a2/b0/41/435ee4e918f91918be41529283c5ff86cd010f02e7525aecf3\n","Successfully built kobert\n","Installing collected packages: kobert\n","Successfully installed kobert-0.1.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":928,"referenced_widgets":["7a53027283234d7a8641fecbcdeb7a79","5a6d2bbfe6cf4020aaf15cab80605504","5c79847a595a4d07a79c7515bab018c6","1584c71aced14e208d251dfce3b1bc53","429eef2bccd949d18c496768248d52d0","39ff263fe7604ab1adeaecccf5b79c0e","a152f69fd91348d1b568ba3c7cd7a45d","9089e76a3e8d42ae98b44a42dd1ef3ed"]},"id":"xoGzCWpisbK9","outputId":"676dc2a7-0bc1-45af-f9b8-23eb6c390367"},"source":["import pandas as pd\n","import os\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optims\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","import subprocess\n","\n","device = torch.device(\"cuda:0\")\n","\n","bertmodel, vocab = get_pytorch_kobert_model()\n","\n","\n","\n","dataset_train = nlp.data.TSVDataset('/content/drive/MyDrive/애쓰는 감자/data/mid_data/train_mid_1.txt', field_indices=[1,2], num_discard_samples=1)\n","# dataset_test = nlp.data.TSVDataset('/content/tst.txt', field_indices=[1,2], num_discard_samples=1)\n","\n","\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","\n","\n","\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","\n","\n","max_len = 256 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n","batch_size = 32\n","warmup_ratio = 0.1\n","num_epochs = 2\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate = 5e-5\n","\n","\n","data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n","\n","\n","# pytorch용 DataLoader 사용\n","train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n","# test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)\n","\n","\n","\n","class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes = 77, # softmax 사용 <- binary일 경우는 2\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)\n","\n","\n","model = BERTClassifier(bertmodel, dr_rate=0.2).to(device)\n","# model = torch.load('weights/last_kobert_pytorch_model_big2s.pt')\n","\n","# if torch.cuda.device_count() > 1:\n","    # model = nn.DataParallel(model)\n","# model = nn.DataParallel(model, output_device=[0,1])\n","# model.to(device)\n","\n","\n","# Prepare optimizer and schedule (linear warmup and decay)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","\n","\n","\n","# 옵티마이저 선언\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss() # softmax용 Loss Function 정하기 <- binary classification도 해당 loss function 사용 가능\n","\n","t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)\n","\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n","\n","# lr_scheduler = optims.lr_scheduler.CosineAnnealingLR(optimizer,T_max=0.1,eta_min=0.0001)\n","\n","\n","# 학습 평가 지표인 accuracy 계산 -> 얼마나 타겟값을 많이 맞추었는가\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc\n","\n","\n","\n","# 모델 학습 시작\n","for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    best_acc = 0.0\n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # gradient clipping\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        # lr_scheduler.step()\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","        if train_acc > best_acc:\n","            best_acc = train_acc\n","        torch.save(model, '/content/drive/MyDrive/애쓰는 감자/data/minz_data/weight/best_kobert_mid_1.pt')\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","\n","torch.save(model, '/content/drive/MyDrive/애쓰는 감자/data/minz_data/weight/last_kobert_mid_1.pt')\n","\n","\n","# # model.eval() # 평가 모드로 변경\n","    \n","# # for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","# #     token_ids = token_ids.long().to(device)\n","# #     segment_ids = segment_ids.long().to(device)\n","# #     valid_length= valid_length\n","# #     label = label.long().to(device)\n","# #     out = model(token_ids, valid_length, segment_ids)\n","# #     test_acc += calc_accuracy(out, label)\n","# # print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["using cached model\n","using cached model\n","using cached model\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:142: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a53027283234d7a8641fecbcdeb7a79","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=17837.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch 1 batch id 1 loss 4.381725311279297 train acc 0.03125\n","epoch 1 batch id 201 loss 4.042562484741211 train acc 0.09235074626865672\n","epoch 1 batch id 401 loss 3.3187170028686523 train acc 0.12951995012468828\n","epoch 1 batch id 601 loss 3.164393663406372 train acc 0.18625207986688852\n","epoch 1 batch id 801 loss 2.387950897216797 train acc 0.24804931335830213\n","epoch 1 batch id 1001 loss 2.341085910797119 train acc 0.29879495504495507\n","epoch 1 batch id 1201 loss 2.277179718017578 train acc 0.34120004163197337\n","epoch 1 batch id 1401 loss 1.6469964981079102 train acc 0.376628301213419\n","epoch 1 batch id 1601 loss 1.7877960205078125 train acc 0.4069331667707683\n","epoch 1 batch id 1801 loss 1.5822762250900269 train acc 0.43104525263742366\n","epoch 1 batch id 2001 loss 1.3391884565353394 train acc 0.4528673163418291\n","epoch 1 batch id 2201 loss 1.2846425771713257 train acc 0.4721007496592458\n","epoch 1 batch id 2401 loss 1.5218197107315063 train acc 0.48832517700957934\n","epoch 1 batch id 2601 loss 1.7728384733200073 train acc 0.5017781622452903\n","epoch 1 batch id 2801 loss 1.0809440612792969 train acc 0.5145372188504106\n","epoch 1 batch id 3001 loss 1.2264868021011353 train acc 0.5258143118960347\n","epoch 1 batch id 3201 loss 1.4029759168624878 train acc 0.5356333958138082\n","epoch 1 batch id 3401 loss 1.3876943588256836 train acc 0.5449592031755366\n","epoch 1 batch id 3601 loss 1.1057497262954712 train acc 0.553223063038045\n","epoch 1 batch id 3801 loss 0.9807466864585876 train acc 0.5612338858195212\n","epoch 1 batch id 4001 loss 1.054098129272461 train acc 0.5678658460384903\n","epoch 1 batch id 4201 loss 0.6788020133972168 train acc 0.5742233991906689\n","epoch 1 batch id 4401 loss 1.4855231046676636 train acc 0.5798611111111112\n","epoch 1 batch id 4601 loss 1.4599140882492065 train acc 0.5855113018908933\n","epoch 1 batch id 4801 loss 0.657661497592926 train acc 0.5904368881483024\n","epoch 1 batch id 5001 loss 0.7478562593460083 train acc 0.5948310337932413\n","epoch 1 batch id 5201 loss 0.8818315267562866 train acc 0.5992958084983657\n","epoch 1 batch id 5401 loss 1.1382839679718018 train acc 0.6031926957970746\n","epoch 1 batch id 5601 loss 1.0050606727600098 train acc 0.6069172915550795\n","epoch 1 batch id 5801 loss 0.8455675840377808 train acc 0.6107890880882606\n","epoch 1 batch id 6001 loss 0.8076689839363098 train acc 0.6143976003999333\n","epoch 1 batch id 6201 loss 1.3000038862228394 train acc 0.6173399451701338\n","epoch 1 batch id 6401 loss 0.8329896926879883 train acc 0.6204889860959225\n","epoch 1 batch id 6601 loss 1.5635242462158203 train acc 0.6232389031964853\n","epoch 1 batch id 6801 loss 1.2195731401443481 train acc 0.6261808925158066\n","epoch 1 batch id 7001 loss 1.3400423526763916 train acc 0.6287941008427368\n","epoch 1 batch id 7201 loss 0.8391056060791016 train acc 0.6313576239411193\n","epoch 1 batch id 7401 loss 0.9082101583480835 train acc 0.6336010336441021\n","epoch 1 batch id 7601 loss 1.6818389892578125 train acc 0.635800388106828\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SUUWFOeHjjYp"},"source":["<font color = pink> test 예측하기 </font>"]},{"cell_type":"code","metadata":{"id":"VeNPgKSkOSy0","colab":{"base_uri":"https://localhost:8080/","height":219,"referenced_widgets":["aa837b8491f74722a804a7f593f1ccde","6e558e01985746008c39fb6c1ec24515","ecb992fa079f41f1b29445eb6d27a651","23c9116f4cee462bbb283eb62811d5cc","4820e356a8d8420ea91693587fca03c3","0f2490faf3db4c36b52b5e7f003e239b","ae0d299460d24e2885e7ab2218669e5d","eb9b87b0209e43ecafbd274352d5a598"]},"executionInfo":{"status":"ok","timestamp":1621486499012,"user_tz":-540,"elapsed":111373,"user":{"displayName":"곽민지","photoUrl":"","userId":"00094324617217372282"}},"outputId":"7f80cf5d-ccc6-498b-fbac-95f546beb0af"},"source":["# test 예측하기\n","device = torch.device('cuda:0')\n","model = torch.load('/content/drive/MyDrive/애쓰는 감자/data/minz_data/best_kobert_model_small.pt')\n","model.to(device)\n","# model = nn.DataParallel(model, output_device=[0,1])\n","bertmodel, vocab = get_pytorch_kobert_model()\n","model.eval() # 평가 모드로 변경\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc\n","\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","max_len = 256 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n","batch_size = 32\n","warmup_ratio = 0.1\n","num_epochs = 2\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate = 5e-5\n","dataset_test = nlp.data.TSVDataset('/content/drive/MyDrive/애쓰는 감자/data/new_data/bert_test.txt', field_indices=[1,2], num_discard_samples=1)\n","data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n","test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)\n","test_acc = 0.0\n","df = pd.DataFrame(columns=['pred','label'])\n","pred = np.array([])\n","answer = np.array([])\n","for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","    token_ids = token_ids.long().to(device)\n","    segment_ids = segment_ids.long().to(device)\n","    valid_length= valid_length\n","    label = label.long().to(device)\n","    out = model(token_ids, valid_length, segment_ids)\n","    _,max_idx = torch.max(out,1)\n","    pred = np.append(pred,max_idx.cpu().detach().tolist())\n","    # answer = np.append(answer,label.cpu().detach().tolist())\n","    # test_acc += calc_accuracy(out, label)\n","    # print(len(pred))\n","df['pred'] = pred\n","# df['label'] = answer\n","\n","df.to_csv('/content/drive/MyDrive/애쓰는 감자/data/minz_data/test_small.csv',index=False)\n","# print(\"test acc {}\".format(test_acc / (batch_id+1)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["using cached model\n","using cached model\n","using cached model\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa837b8491f74722a804a7f593f1ccde","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=599.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rCE1gGbqj6pZ"},"source":["<font color = pink> val 예측하기 </font>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219,"referenced_widgets":["31c519addcbf4094a1e3d6bbbe23b100","b3fb9eaf49ba4f57a17a6db2bd7c5125","9d51d38e34224d4fb2f4449f38fc66bd","4377fbe447d44c3e9cc8106f5ed2a43c","c7995b2b80f645feab9a5542a4ee04a9","98cd9383d93044a5a6aa1e81b1d52259","59a2c70bde234fe6a8cfcaadc7cc9130","361f4e2117284516afbd0f3ce1cbe5ad"]},"id":"TUuOjtmxGd7s","executionInfo":{"status":"ok","timestamp":1621508947430,"user_tz":-540,"elapsed":2407431,"user":{"displayName":"곽민지","photoUrl":"","userId":"00094324617217372282"}},"outputId":"5f7190b5-07d0-4308-83d5-2944c261f04a"},"source":["# validation 예측하기\n","\n","device = torch.device('cuda:0')\n","model = torch.load('/content/drive/MyDrive/애쓰는 감자/data/minz_data/weight/best_kobert_model_small.pt')\n","model.to(device)\n","# model = nn.DataParallel(model, output_device=[0,1])\n","bertmodel, vocab = get_pytorch_kobert_model()\n","model.eval() # 평가 모드로 변경\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc\n","\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","max_len = 256 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n","batch_size = 32\n","warmup_ratio = 0.1\n","num_epochs = 2\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate = 5e-5\n","dataset_test = nlp.data.TSVDataset('/content/drive/MyDrive/애쓰는 감자/data/minz_data/val_small_pred.txt', field_indices=[1,2], num_discard_samples=1)\n","data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n","test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)\n","test_acc = 0.0\n","df = pd.DataFrame(columns=['pred','label'])\n","pred = np.array([])\n","answer = np.array([])\n","for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","    token_ids = token_ids.long().to(device)\n","    segment_ids = segment_ids.long().to(device)\n","    valid_length= valid_length\n","    label = label.long().to(device)\n","    out = model(token_ids, valid_length, segment_ids)\n","    _,max_idx = torch.max(out,1)\n","    pred = np.append(pred,max_idx.cpu().detach().tolist())\n","    # answer = np.append(answer,label.cpu().detach().tolist())\n","    # test_acc += calc_accuracy(out, label)\n","    # print(len(pred))\n","df['pred'] = pred\n","# df['label'] = answer\n","\n","df.to_csv('/content/drive/MyDrive/애쓰는 감자/data/minz_data/val_small_pred.csv',index=False)\n","# print(\"test acc {}\".format(test_acc / (batch_id+1)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["using cached model\n","using cached model\n","using cached model\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31c519addcbf4094a1e3d6bbbe23b100","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=15263.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IeOIgPwzUkFv"},"source":[""],"execution_count":null,"outputs":[]}]}