{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**코드에 언급되어 있지 않은 데이터들을 불러오는 것은 최종코드에서 추출한 데이터들을 따로 저장해서 불러왔습니다.**\n",
    "\n",
    "**사용자의 편의상 가장 간단한 방법이라 생각하였습니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mxnet\n",
    "!pip install gluonnlp\n",
    "!pip install sentencepiece\n",
    "!pip install transformers==3\n",
    "!pip install torch\n",
    "\n",
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!set -x \\\n",
    "&& pip install konlpy \\\n",
    "&& curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash -x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,  ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from konlpy.tag import Mecab\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data & preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Bert용 데이터로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'D:\\KED\\submit\\본선검증용.xlsx')\n",
    "data.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "kedcd = data.KEDCD.unique()\n",
    "data = data[data.notnull()]\n",
    "data['BZ_PPOS_ITM_CTT'] = data['BZ_PPOS_ITM_CTT'].str.replace('[^\\w\\s]', '', regex=True)\n",
    "data['BZ_PPOS_ITM_CTT'] = data['BZ_PPOS_ITM_CTT'].str.replace(\"^[0-9]\",'')\n",
    "dic = data.groupby(['KEDCD'])['BZ_PPOS_ITM_CTT'].apply(lambda x: x.values.tolist()).to_dict()\n",
    "test_df = pd.DataFrame(columns=['KEDCD','BZ_PPOS_ITM_CTT'])\n",
    "test_df['KEDCD'] = dic.keys()\n",
    "test_df['BZ_PPOS_ITM_CTT'] = dic.values()\n",
    "test_df['BZ_PPOS_ITM_CTT'] = test_df['BZ_PPOS_ITM_CTT'].apply(lambda ls: [x for x in ls if str(x) != 'nan'])\n",
    "test_df['BZ_PPOS_ITM_CTT']=test_df['BZ_PPOS_ITM_CTT'].apply(lambda x: ''.join(x))\n",
    "test_df2 = test_df.reset_index()\n",
    "test_df2.drop('KEDCD',axis=1,inplace=True)\n",
    "test_df2.columns = ['KEDCD','BZ_PPOS_ITM_CTT']\n",
    "test_df2['KSIC10_BZC_CD'] = 0\n",
    "test_df2.drop('KEDCD',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2.to_csv('submit/Bert_final_test.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM용 데이터로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('inference_data/X_train_new_se.pickle','rb') as f:\n",
    "    X_train= pickle.load(f)   # 기존에 학습용 데이터를 불러옴(편의상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/content/drive/MyDrive/애쓰는 감자/submit/Bert_final_test.txt',sep='\\t')\n",
    "\n",
    "mecab = Mecab()\n",
    "stop_words = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다','을']\n",
    "\n",
    "\n",
    "X_test = []\n",
    "\n",
    "text = list(test['BZ_PPOS_ITM_CTT'])\n",
    "\n",
    "for i in tqdm(range(len(text))):\n",
    "    temp_X = []\n",
    "    temp_X = mecab.nouns(text[i]) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stop_words] # 불용어 제거\n",
    "    # temp_X = [word for word in temp_X if len(word) > 1]\n",
    "    X_test.append(temp_X)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "threshold = 11\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "tokenizer = Tokenizer(num_words = vocab_size) # num_words = vocab_size\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "token_X_test = tokenizer.texts_to_sequences(X_test)\n",
    "max_len = 300 \n",
    "pad_X_test = pad_sequences(token_X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/winston1214/project/master/KED/bert_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python bert_test.py --source Bert_final_test.txt --weights weight/final_big.pt --batch 32 --device 0 --save_csv_name final_big.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 중분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python bert_test.py --source Bert_final_test.txt --weights weight/final_mid.pt --batch 32 --device 0 --save_csv_name final_mid.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 소분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python bert_test.py --source Bert_final_test.txt --weights weight/final_small.pt --batch 32 --device 0 --save_csv_name final_small.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 세세분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python bert_test.py --source Bert_final_test.txt --weights weight/final_sese.pt --batch 32 --device 0 --save_csv_name final_sese.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 세분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 18658\n",
    "max_len = 300\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(vocab_size, 32, input_length=max_len))\n",
    "model4.add(Dropout(0.3))\n",
    "model4.add(Conv1D(32, 5, activation='relu'))\n",
    "model4.add(Conv1D(32, 5, activation='relu'))\n",
    "model4.add(MaxPooling1D(pool_size=4))\n",
    "model4.add(LSTM(32))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dense(491, activation='softmax'))\n",
    "model4.summary()\n",
    "model4 = load_model( '/content/drive/MyDrive/애쓰는 감자/submit/weight/se_model_0520')\n",
    "pred4 = model4.predict(pad_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과값 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big = pd.read_csv('final_big.csv')\n",
    "mid = pd.read_csv('final_mid.csv')\n",
    "small = pd.read_csv('final_small.csv')\n",
    "sese = pd.read_csv('final_sese.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대분류 라벨인코딩 -> 예측 코드로 디코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "dic = {}\n",
    "for k,v in zip([i for i in range(21)],string.ascii_uppercase):\n",
    "    dic[k] = v\n",
    "submit['KEDCD'] = test_df2['KEDCD'].astype(str)\n",
    "submit['pred_big'] = big['pred'].astype(int).map(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 중분류 라벨인코딩 -> 예측 코드로 디코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mid = pd.read_csv('preprocessing_data/train_data_mid.csv',dtype={'mid':str})\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_mid['mid'])\n",
    "mapping = dict(zip(range(len(encoder.classes_)), encoder.classes_))\n",
    "mid['label'] = mid['pred'].astype(int).map(mapping)\n",
    "submit['pred_mid'] = mid['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 소분류 라벨인코딩 -> 예측 코드로 디코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small = pd.read_csv('preprocessing_data/train_data_small.csv',dtype={'small':str})\n",
    "encoder = LabelEncoder() # encoding\n",
    "encoder.fit(train_small['small'])\n",
    "mapping = dict(zip(range(len(encoder.classes_)), encoder.classes_)) # decoding\n",
    "small['label'] = small['pred'].astype(int).map(mapping)  # decoding mapping\n",
    "submit['pred_small'] = small['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 세세분류 라벨인코딩 -> 예측 코드로 디코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sese = pd.read_csv('preprocessing_data/train_data_sese.csv',dtype={'sese':str})\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_sese['sese'])\n",
    "mapping = dict(zip(range(len(encoder.classes_)), encoder.classes_))\n",
    "sese['label'] = sese['pred'].astype(int).map(mapping)\n",
    "submit['sese'] = sese['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 세분류 OneHotEncoding -> 예측코드로 디코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_se = pd.read_csv('train_data_se.csv',dtype={'KEDCD':str,'BZ_PPOS_ITM_CTT':str,'KSIC10_BZC_CD':str,'mid':str,'Big':str,'small':str,'se':str})\n",
    "ls = train_se['se'].unique().tolist()\n",
    "final_ls = []\n",
    "for i in ls:\n",
    "    if len(i)==4:\n",
    "        final_ls.append(i)\n",
    "final_ls = sorted(final_ls)\n",
    "pred_se_test = []\n",
    "for i in pred4:\n",
    "    pred_idx = np.argmax(i)\n",
    "    pred_se_test.append(final_ls[pred_idx])\n",
    "submit['pred_se'] = pred_se_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 예측 코드 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.columns = ['KEDCD','big','mid','small','sese','se']\n",
    "\n",
    "final_mid = []\n",
    "final_small = []\n",
    "# final_se = []\n",
    "for i in range(len(submit)):\n",
    "    # 중분류\n",
    "    ls = []\n",
    "    mid = submit.iloc[i]['mid']\n",
    "    small = submit.iloc[i]['small']\n",
    "    se = submit.iloc[i]['se']\n",
    "    sese = submit.iloc[i]['sese']\n",
    "    ls.append(mid)\n",
    "    ls.append(small[:-1])\n",
    "    ls.append(se[:-2])\n",
    "    ls.append(sese[:-3])\n",
    "    counting = pd.Series(ls).value_counts()\n",
    "    if counting.values[0] == 1:\n",
    "        final_mid.append(mid)\n",
    "    elif counting.values[0] == 2:\n",
    "        if counting.values[0] == 2:\n",
    "            final_mid.append(mid)\n",
    "    else:\n",
    "        final_mid.append(counting.index[0])\n",
    "    # 소분류\n",
    "    ls2 = []\n",
    "    ls2.append(small[-1])\n",
    "    ls2.append(se[-2])\n",
    "    ls2.append(sese[-3])\n",
    "    counting2 = pd.Series(ls2).value_counts()\n",
    "    if counting2.values[0] == 1:\n",
    "        final_small.append(small[-1])\n",
    "    else:\n",
    "        final_small.append(counting2.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['final_mid'] = final_mid\n",
    "submit['final_small'] = final_small\n",
    "submit['pred'] = submit['big']+submit['final_mid'] + submit['final_small'].str[-1] + submit['se'].str[-1] + submit['sese'].str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submit = pd.read_excel('[공모전]팀이름_본선.xlsx',dtype={'KEDCD':str})\n",
    "final_submit.info()\n",
    "df = pd.merge(final_submit,submit,on='KEDCD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('submit/[공모전]본선_애쓰는감자.xlsx',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
